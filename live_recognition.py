import numpy as np
import sounddevice as sd
import tensorflow as tf
import librosa
import json
import time
import paho.mqtt.client as mqtt

# --- KONFIGURACJA MQTT ---
MQTT_BROKER = "broker.emqx.io"
MQTT_PORT = 1883
MQTT_TOPIC = "AI/voice"

# --- INICJALIZACJA MQTT ---
client = mqtt.Client()
try:
    client.connect(MQTT_BROKER, MQTT_PORT, 60)
    print(f"Po≈ÇƒÖczono z brokerem MQTT: {MQTT_BROKER}")
except Exception as e:
    print(f"B≈ÇƒÖd po≈ÇƒÖczenia MQTT: {e}")

# --- KONFIGURACJA MODELU ---
MODEL_PATH = "voice_command_model.h5"
CLASSES_PATH = "classes.npy"
CONFIG_PATH = "config.json"

# Parametry audio (muszƒÖ byƒá identyczne jak w create_melspectograms.py)
TARGET_SR = 16000
DURATION = 1.0
N_MELS = 80
N_FFT = 400
HOP_LENGTH = 160

# Parametry detekcji
THRESHOLD = 0.85  # Minimalna pewno≈õƒá, by uznaƒá komendƒô
SILENCE_THRESHOLD = 0.02  # Minimalna g≈Ço≈õno≈õƒá (amplituda), by w og√≥le analizowaƒá (zmniejszono z 0.1)
COOLDOWN = 1.0  # Czas blokady po wykryciu (s)

# --- ≈ÅADOWANIE ZASOB√ìW ---
print("Wczytywanie modelu...")
model = tf.keras.models.load_model(MODEL_PATH)
classes = np.load(CLASSES_PATH)
# config = json.load(open(CONFIG_PATH)) # Opcjonalne

# --- ZMIENNE STANU ---
last_digit = None
last_digit_time = 0
TIMEOUT = 5.0
last_recognition_time = 0


def select_audio_device():
    """Wyb√≥r mikrofonu."""
    print("\n--- DOSTƒòPNE URZƒÑDZENIA AUDIO ---")
    devices = sd.query_devices()
    input_devices = []
    for i, dev in enumerate(devices):
        if dev['max_input_channels'] > 0:
            input_devices.append(i)
            print(f"ID {i}: {dev['name']}")

    default = sd.default.device[0]
    choice = input(f"\nPodaj ID (Enter = domy≈õlne {default}): ")
    return int(choice) if choice.strip() else default


def process_command(cmd, probability):
    """Logika sterowania."""
    global last_digit, last_digit_time

    digits_map = {
        "zero": "0", "one": "1", "two": "2", "three": "3", "four": "4",
        "five": "5", "six": "6", "seven": "7", "eight": "8", "nine": "9"
    }

    current_time = time.time()

    # 1. Wykryto cyfrƒô
    if cmd in digits_map:
        last_digit = digits_map[cmd]
        last_digit_time = current_time
        print(f"   [!] Zapamiƒôtano: {last_digit}. Czekam na 'on'/'off'...")

    # 2. Wykryto komendƒô ON/OFF
    elif cmd in ["on", "off"]:
        if last_digit is not None:
            if current_time - last_digit_time < TIMEOUT:
                payload = f"{last_digit} {cmd}"
                client.publish(MQTT_TOPIC, payload)
                print(f"   >>> WYS≈ÅANO MQTT: {payload} <<<")
                last_digit = None
            else:
                print("   [Timeout] MinƒÖ≈Ç czas na komendƒô. Powt√≥rz cyfrƒô.")
                last_digit = None
        else:
            print("   [B≈ÇƒÖd] Najpierw podaj cyfrƒô.")


def preprocess_audio(audio_data):
    """
    Kluczowa funkcja z zabezpieczeniem przed ciszƒÖ.
    Zwraca None, je≈õli sygna≈Ç to tylko szum t≈Ça.
    """
    # 1. Tworzenie mel-spektrogramu
    mel = librosa.feature.melspectrogram(
        y=audio_data, sr=TARGET_SR, n_fft=N_FFT,
        hop_length=HOP_LENGTH, n_mels=N_MELS, power=2.0
    )
    log_mel = librosa.power_to_db(mel, ref=np.max)

    # 2. ZABEZPIECZENIE: Sprawdzenie "p≈Çasko≈õci" sygna≈Çu (szumu)
    # Cisza ma bardzo ma≈Çe odchylenie standardowe.
    # Je≈õli znormalizujemy ciszƒô, wyjdƒÖ losowe wzory ("six"/"eight").
    current_std = np.std(log_mel)

    # Je≈õli odchylenie jest poni≈ºej 3.0 dB, uznajemy to za szum t≈Ça i odrzucamy
    if current_std < 3.0:
        return None

    # 3. Normalizacja (tylko je≈õli sygna≈Ç jest znaczƒÖcy)
    mean = np.mean(log_mel)
    std = current_std + 1e-9
    log_mel = (log_mel - mean) / std

    # 4. Dopasowanie kszta≈Çtu (Batch, Height, Width, Channels)
    input_data = log_mel[np.newaxis, ..., np.newaxis]
    return input_data


def start_live_recognition():
    global last_recognition_time
    device_id = select_audio_device()

    # Bufor na 1 sekundƒô
    window_samples = int(TARGET_SR * DURATION)
    # Przesuniƒôcie o 0.2 sekundy
    step_samples = int(TARGET_SR * 0.2)

    audio_buffer = np.zeros(window_samples, dtype=np.float32)

    print(f"\nNas≈Çuchujƒô... (Klasy: {classes})")
    print("M√≥w do mikrofonu.\n")

    with sd.InputStream(device=device_id, channels=1, samplerate=TARGET_SR,
                        blocksize=step_samples, dtype='float32') as stream:
        while True:
            # Pobierz nowe pr√≥bki
            new_data, overflow = stream.read(step_samples)
            if overflow: print("!", end="", flush=True)

            # Aktualizacja bufora (FIFO)
            audio_buffer = np.roll(audio_buffer, -step_samples)
            audio_buffer[-step_samples:] = new_data.flatten()

            # --- WSTƒòPNA FILTRACJA (Bramka szum√≥w - amplituda) ---
            # Je≈õli jest absolutna cisza, nawet nie pr√≥buj liczyƒá spektrogramu
            if np.max(np.abs(audio_buffer)) < SILENCE_THRESHOLD:
                continue

            # --- PRZETWARZANIE I ZABEZPIECZENIE PRZED SZUMEM ---
            input_tensor = preprocess_audio(audio_buffer)

            # Je≈õli preprocess_audio zwr√≥ci≈Ço None (bo wykry≈Ço p≈Çaski szum), pomi≈Ñ
            if input_tensor is None:
                continue

            # --- PREDYKCJA ---
            prediction = model.predict(input_tensor, verbose=0)
            idx = np.argmax(prediction)
            prob = prediction[0][idx]
            command = classes[idx]

            # Wy≈õwietlanie wszystkiego co ma sensownƒÖ pewno≈õƒá (dla debugowania)
            if prob > 0.5:
                # print(f"Debug: {command} ({prob:.2f})") # Odkomentuj by widzieƒá co model "my≈õli"
                pass

            # --- DECYZJA ---
            current_time = time.time()
            if (prob > THRESHOLD and
                    command not in ["silence", "unknown"] and
                    (current_time - last_recognition_time > COOLDOWN)):
                print(f"\nüéôÔ∏è Wykryto: '{command}' ({prob:.2f})")
                process_command(command, prob)
                last_recognition_time = current_time


if __name__ == "__main__":
    try:
        start_live_recognition()
    except KeyboardInterrupt:
        print("\nZatrzymano.")
        client.disconnect()